{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from math import log\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New in version 2.0.0.\n",
    "classmethod train(rdd, k=4, maxIterations=20, minDivisibleClusterSize=1.0, seed=-1888008604)\n",
    "     # Runs the bisecting k-means algorithm return the model.\n",
    "     Parameters:\t\n",
    "               rdd – Training points as an RDD of Vector or convertible sequence types.\n",
    "               k – The desired number of leaf clusters. The actual number could be smaller if there are no divisible leaf clusters. (default: 4)\n",
    "               maxIterations – Maximum number of iterations allowed to split clusters. (default: 20)\n",
    "               minDivisibleClusterSize – Minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. (default: 1)\n",
    "               seed – Random seed value for cluster initialization. (default: -1888008604 from classOf[BisectingKMeans].getName.##)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([0.0,0.0, 1.0,1.0, 9.0,8.0, 8.0,9.0]).reshape(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans.train(\n",
    "        sc.parallelize(data), 2, maxIterations=10, initializationMode=\"random\",\n",
    "                       seed=50, initializationSteps=5, epsilon=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([0.0, 0.0])) == model.predict(np.array([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0000000000000004"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.computeCost(sc.parallelize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,1),(2,2),(3,3),(4,4),(5,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def runKmeans(data, sample_dataset, k, count):\n",
    "    clusters = KMeans.train(sample_dataset, k, maxIterations=1, initializationMode=\"kmean++\")\n",
    "    cost = clusters.computeCost(data)\n",
    "    finalcost = cost/data.count()\n",
    "    return finalcost\n",
    "runKmeans(rdd, sc.parallelize([(1,1),(2,2),(3,3)]), 3, rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 10\n",
    "n2 = 100\n",
    "e1 = 10000\n",
    "e2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def computeIntrinsicDimension(n1, e1, n2, e2):\n",
    "    a1 = (log(n2) - log(n1))\n",
    "    a2 = (log(e1) - log(e2))\n",
    "    d = 2*a1/a2\n",
    "    return d\n",
    "print (computeIntrinsicDimension (n1, e1, n2, e2))\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dataframe to rdd\n",
    "rdd = df.rdd.map(tuple)\n",
    "or\n",
    "\n",
    "rdd = df.rdd.map(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.parquet(file_path)\n",
    "df = spark.read.parquet(\"/Users/user/Desktop/PSDS/edX/UCSandiago_MicroMasters/BigDataAUSpark/wk5_6/hw5-small.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID_10000_10_200': 1.5255694332344738, 'ID_10000_200_700': 1.2442099315834783, 'ID_10000_700_2000': 1.2585590236134818, 'ID_20000_10_200': 1.577912839175332, 'ID_20000_200_700': 1.2111828964759064, 'ID_20000_700_2000': 1.152710266127354}\n"
     ]
    }
   ],
   "source": [
    "def run(df):\n",
    "    for each in df.columns:\n",
    "        df = df.withColumn(each, df[each].cast(FloatType()))\n",
    "    rdd = df.rdd.map(list)\n",
    "    sample1 = sc.parallelize(rdd.takeSample(0, 10000)) #10k\n",
    "    sample2 = sc.parallelize(rdd.takeSample(0, 20000)) #20k\n",
    "    results ={}\n",
    "    k = [10, 200, 700, 2000]\n",
    "    for num, s in zip(['10000', '20000'], [sample1, sample2]): # s is sample\n",
    "        MSD_list = [runKmeans(rdd, s, per_k, rdd.count()) for per_k in k] #mean square distance\n",
    "        d1, d2, d3, d4 = MSD_list\n",
    "        k1, k2, k3, k4 = k\n",
    "        results['ID_{0}_10_200'.format(num)] = computeIntrinsicDimension(k1, d1, k2, d2)\n",
    "        results['ID_{0}_200_700'.format(num)] = computeIntrinsicDimension(k2, d2, k3, d3)\n",
    "        results['ID_{0}_700_2000'.format(num)] = computeIntrinsicDimension(k3, d3, k4, d4)\n",
    "    return results\n",
    "#print(run(df))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
